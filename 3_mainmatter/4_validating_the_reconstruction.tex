% !TEX root=../main.tex

\chapter[Validating and evaluating the efficiency of Pandora reconstruction]{Validation and evaluation of the efficiency of the Pandora reconstruction for track-like event topologies}\label{chap:methods}

Pandora-based event reconstruction pipeline, as described in \autoref{sec:TPC_reco_gen}, is central to the physics analyses conducted within the ICARUS and SBN collaborations. Its use across the LArTPC community ensures it is a set of tools that is actively maintained, improved and updated as new algorithms suitable for the event reconstruction are developed. Any improvement made by each collaboration sharing this tool is thus an improvement that can be exploited for analyses by other collaborations that rely on the same reconstruction tools.

Pandora framework in ICARUS employs the same structure inherited from the MicroBooNE collaboration, shared also with the SBND experiment. Especially important for the success of the SBN program is to have the same reconstruction infrastructure for both experiments in order to make systematic uncertainties related to the reconstruction as less relevant as possible.  Recent efforts were made to better align Pandora reconstruction paths between ICARUS and SBND, with the inclusion of shower-targeted algorithms, aiming at increasing efficiency in the reconstruction of shower-like particle clusters. 

Moving in the direction of improving the reconstruction performance for the ICARUS detector, this work focuses on the implementation of a set of tools that enable the use of true Monte Carlo information to alter the reconstruction output of the different stages. This comprehensive set of tools has already been tested and used in the context of the SBND reconstruction, as well as for other Pandora-based reconstruction pipelines \cite{Mawby:2023nws, Nguyen:2023_cheatingPandora}. These tools allow to investigate the impact of each step of Pandora reconstruction by altering the reconstructed objects using simulated events, where the true information derived from Monte Carlo is available. Using these altered reconstruction tools provides a strategy to validate the contribution of each algorithm to the entire reconstruction chain. 

However, developing reconstruction improvements in isolation is difficult and does not always provide clear quantification of the impact made. 
Therefore, I focused my studies on a precise analysis in order to identify the pain points in the reconstruction process. 

The ICARUS collaboration, using the collected data of the second physics data-taking campaign, performed as a standalone one-detector campaign, is moving towards a first oscillation analysis, preliminary to the two-detector oscillation analysis expected with the data collected by the SBN programme. This first analysis is targeted at studying the muon neutrino disappearance channel. The goals of this analysis are to contribute to world knowledge of potential eV-scale sterile neutrino oscillation in a timely way and to demonstrate the capability of ICARUS to produce high-quality physics results.

The $\PGnGm$-disappearance analysis  the collaboration is now conducting uses a precise event topology, namely the muon neutrino charged current quasi-elastic topology, with a single muon in the final state, any positive number of protons, zero visible electromagnetic showers (both from electrons and from photons) and zero charged pions. This selection is referred to as $\PGnGm$CC N$\Pp$ QE \cite{particles8010018, arteroponsStudyReconstructionNuMuCC}.  

The chapter articulates as follows. The first section (\autoref{sec:dataSample_and_selection}) is devoted to describing in detail the event selection and the data sample used to perform this work. The second section (\autoref{sec:cheatingPandora}) delves into the details of the tools used for this work: what it means to ``\emph{cheat}'' Pandora reconstruction and how it is performed for all the stages involved in the event reconstruction; this section is also devoted to validating the tools performance. The two final sections (\autoref{sec:methods} and \autoref{sec:vertexResults}) describe the methods followed to exploit the tools introduced in the previous sections to extract the performance of all the steps in the reconstruction and pinpoint the stages that require the most action. 

\section{Data sample and selection} \label{sec:dataSample_and_selection}

The $1\PGm N\Pp$ event topologies used in the muon neutrino disappearance study are selected automatically using a thoroughly tuned and tested selection procedure \cite{particles8010018}. This section provides a comprehensive description of the event selection procedure adopted for the present analysis, highlighting the decisions behind all the specific selection cuts.

\paragraph{CRT-PMT match} Exploiting the CRT-PMT match \cite{ICARUS:2025_nuMuTechNote} performed in the event reconstruction, non-contained neutrino interactions or cosmic-ray particles are rejected. The cut searches for coincidences in the \SI{150}{\ns} gate between CRT hits and an optical flash. Only optical flashes in the beam spill gate window $\qty[0,1.6]\ \si{\us}$, i.e. compatible with neutrino/beam-induced events, are considered. This first selection contributes to reducing the huge amount of cosmic ray interactions in the detector, thus slightly improving on the selection purity ($+\SI{0.1}{\percent}$)

\paragraph{Optical flash barycenter match} The light information coming from the PMTs is also used to improve the selection by exploiting the 3D event reconstruction they allow. Using the light from the triggering PMT flash, the barycenter of the light hit is computed. The barycenter is computed as the mean of optical hits, weighted by the integral of the signal on each optical hit, \begin{equation}
    \vec{x} = \frac{\sum_i \vec x_i \times \mathrm{PE}_i}{\sum_i \mathrm{PE}_i}.  \label{eq:hitLighBarycenter}
\end{equation} Here $\vec x_i$ represents the position of the PMT producing the $i$-th hit, with a signal integral of $\mathrm{PE}_i$ photoelectrons collected by the $i$-th photomultiplier. Similarly is defined also the charge barycenter of a slice. Considering all the hits in one slice and weighting their position on the integral of the signal on each TPC hit, it is possible, with similar math to Eq. \eqref{eq:hitLighBarycenter}, to get the charge barycenter.
This position information is used to reject from the event reconstruction all the slices, i.e. interactions, whose charge barycenter along $z$ was more than \SI{1}{\m} away from the light barycenter along $z$. This cut was made to check compatibility between the event reconstructed in the TPC and the triggering flash in the PMT system; visual scanning efforts demonstrated that this choice very efficiently selected the neutrino interaction slice, while rejecting by a factor of 20 the contamination from cosmic rays crossing the detector outside the spill gate of the beam (``out-of-time'' cosmic interactions).

\paragraph{Vertex and tracks containment} Two basic selection cuts are then applied to the interaction vertex and tracks in the interaction selected by the former cuts. The vertex is required to be inside the fiducial volume (requiring more than \SI{25}{\cm} apart from the lateral TPC walls and 30/\SI{50}{\cm} from the upstream/downstream walls, and an additional cut to go around a dangling cable in the TPC volume) of the ICARUS detector, as outlined in detail in \cite{arteroponsStudyReconstructionNuMuCC}, whereas all the tracks inside the interaction are required to be contained in the active volume of the TPC within \SI{5}{\cm} from all the sides. This request ensures that the PID score described in Eq. \eqref{eq:PID} is correctly computed: for outgoing particles, the $\dv*{E}{x}$ versus residual range is not the same as for contained particles, since it is not defined where the track ends and thus the residual range is shifted; therefore a selection involving outgoing particles would require different metrics for computing the energy loss. 

The interactions that pass these preliminary selection cuts are then analysed in terms of particle content, so each particle in the interaction is classified as such, and interactions with one muon, $N$ protons (with $N\geq1$), zero charged pions and zero showers are selected. I will now define the variable cuts applied to identify each particle species. 

\paragraph{Muon identification} The muon candidate track is identified as the longest reconstructed particle fulfilling the following set of requirements \begin{itemize}
    \item It has to be tagged as a primary particle, so its parent has to be the neutrino associated with the interaction vertex. 
    \item It has to be identified as a track-like object. The BDT implemented at the end of Pandora reconstruction chain \cite{dellepianeBDT} performs a topological classification of  each object inside the interaction, assigning a ``trackscore'' value between zero and one. The more an object is ``track-like'', the closer to one this score has to be. The requirement for muons is a trackscore $\geq 0.5$. 
    \item The reconstructed muon length has to be greater than \SI{50}{\cm} (or equivalently to ${\sim}\SI{105}{\MeV}$ of energy). 
    \item The track starting point has to be at a distance from the reconstructed vertex smaller than\SI{10}{\cm}.
    \item Finally, the PID information is considered (see \autoref{sec:calorimetryAndCalibration} and Eq. \eqref{eq:PID}). This sets the simultaneous requirement to have $\chi^2_\PGm < 30$ (using the $\dv*{E}{x}$ energy loss under the hypothesis that the object is a muon) and $\chi^2_\Pp > 60$ (using the $\dv*{E}{x}$ energy loss under the hypothesis that the object is a proton). 
\end{itemize} In the reconstructed interaction, only one muon is required, passing this selection. 

\paragraph{Proton identification} The identification of the proton follows similar cuts as for the muon\begin{itemize}
    \item The requirement to be tagged as a primary particle.
    \item Based on the track score distribution for protons, which, compared to muons, is slightly shifted towards lower values, a track score cut of 0.4 was set \cite{dellepianeBDT,Campani:2024_neutrinoBDT}.
    \item In order to select only events with visible protons where reconstruction is expected to succeed, a threshold of \SI{2.3}{\cm} is set on the candidate proton track length. This corresponds to an energy threshold of ${\sim}\SI{50}{\mega\electronvolt}$.
     \item The track starting point has has to be at a lower than \SI{10}{\cm} distance from the reconstructed interaction vertex.
     \item PID is then considered, requiring that $\chi^2_\Pp < 100$. 
\end{itemize} Any positive number of protons is required in the present $\PGnGm$CC Np QE selection. 

\paragraph{Pion identification} Pions are identified using very similar selection cuts as protons, with the only exception of requiring $\chi^2_\Pp > 100$, and a deposited energy of \SI{25}{\mega\electronvolt}. Since pionless signatures are considered for the present analysis, if one or more pions are identified within a certain slice, the slice gets rejected. 

\paragraph{Electromagnetic shower identification} Electromagnetic showers produced by photons are identified by means of track score: everything with a value lower than 0.5, which is not classified as a proton (so $\chi^2_\Pp > 100$, among other cuts), is identified as an electromagnetic shower. If more than zero showers are reconstructed and identified  in a given slice, the slice gets rejected. 

The performance of the aforementioned selection have been  evaluated \cite{artero_pons_2024_13841852, particles8010018} by computing the selection efficiency and selection purity, respectively defined as \begin{equation}
    \begin{aligned}
        \mathrm{Efficiency} &= \frac{\text{Selected signal}}{\text{True signal}} = \SI{49}{\percent} \\
        \mathrm{Purity} &= \frac{\text{Selected signal}}{\text{All selected events}} = \SI{84}{\percent}
    \end{aligned}
\end{equation} Here the ``selected signal'' are the reconstructed events that pass the event selection that are also selected as $1\PGm N \Pp$ with the true signal definition, ``true signal'' the events that meet the true signal definition, and ``all selected events'' the reconstructed events that pass the event selection. 

The same selection, with minor changes related to the differences in the reconstruction paradigm, is applied also to data reconstructed with the SPINE framework, described in \autoref{sec:SPINE}. Using this reconstruction paradigm, the efficiency and purity values obtained are, respectively, of ${\sim}\SI{75}{\percent}$ and ${\sim}\SI{80}{\percent}$. 

In parallel with the progress of the analysis, as part of the validation of the tools employed, the efficiencies and purities listed before were tested by using the visual scan technique on a small subsample of the available data. This study confirmed the aforementioned values for both reconstruction frameworks.

\section{\emph{Cheating} Pandora reconstruction} \label{sec:cheatingPandora}

The modular structure of Pandora reconstruction chain allows for great flexibility in the choice and hierarchical arrangement of the tools and algorithms involved in the reconstruction. As described in \autoref{sec:TPC_reco_gen}, and pictured in \autoref{fig:PandoraFastReco}, \ref{fig:PandoraCosmic} and \ref{fig:PandoraNeutrino}, a sequence of multiple algorithms is applied to the reconstructed hits, resulting in reconstructed objects. The steering of Pandora reconstruction chain is performed by XML configuration files where each algorithm is declared and configured. The typical implementation of a single reconstruction algorithm inside Pandora reconstruction framework (described in greater detail in \autoref{sec:TPC_reco_gen}) is illustrated by the example of the TrackClusterCreation as shown below. 

\begin{lstlisting}[style=xmlstyle]
<pandora>
    ...
    <!-- TwoDReconstruction -->
    <algorithm type = "LArClusteringParent">
        <algorithm type = "LArTrackClusterCreation" description = "ClusterFormation"/>
        <InputCaloHitListName>CaloHitListU</InputCaloHitListName>
        <ClusterListName>ClustersU</ClusterListName>
        ...
    </algorithm>
    ...
</pandora>
\end{lstlisting}

This modular approach of Pandora topological event reconstruction can be exploited to allow an algorithm to be replaced with a functionally identical algorithm performing the task with a different approach. This modularity is exploited also to develop tools, which are the core of this work, that, instead of relying on the actual tools to perform the reconstruction task, use the underlying Monte Carlo information. This approach, developed as part of Pandora reconstruction framework, is in this work referred to as ``\emph{cheating}'' of the reconstruction. The powerfulness of this concept has been shown already with reconstruction studies in other experiments, where it is employed to pinpoint the ``failure points'' of the topological event reconstruction and understand the ceiling performance of Pandora reconstruction \cite{Mawby:2023nws, Mawby:2025_FCCee, Nguyen:2023_cheatingPandora}. 

In practical terms, cheating one or multiple algorithms involves replacing the corresponding algorithm in the steering XML configuration file with the respective cheating counterpart \cite{Nguyen:2023_cheatingPandora}. For example, the configuration shown above, guiding the creation of two-dimensional clusters on the $u$ view, is replaced by the CheatingClusterCreation algorithm, which performs the cheating of the cluster creation (which is the first step of both PandoraCosmic and PandoraNeutrino paths; see \autoref{fig:PandoraCosmicNeutrino}), and is listed below

\begin{lstlisting}[style=xmlstyle]
<pandora>
    ...
    <algorithm type = "LArClusteringParent">
        <algorithm type = "LArCheatingClusterCreation" description = "ClusterFormation">
            ...
        </algorithm>
        <InputCaloHitListName>CaloHitListU</InputCaloHitListName>
        <ClusterListName>ClustersU</ClusterListName>
        ...
    </algorithm>
    ...
</pandora>
\end{lstlisting}

The action of cheating an algorithm can be regarded as ensuring the cheated reconstruction step has a \SI{100}{\percent} success rate. Cheating different steps takes into account the action of each step of the reconstruction. For example, referring to the algorithm listed before, its cheated version ensures a flawless reconstruction of two-dimensional clusters on view $u$ with a perfect match between truly generated and reconstructed hits. The next paragraphs are entirely dedicated to delving into the details of how each of the algorithms acts upon the event and thus how cheating is applied specifically to them.

Given the $\PGnGm$CC QE Np sample of events used for this study, the following sections are focused on cheating specifically the PandoraNeutrino reconstruction path. However, it should be noted that most of the algorithms that will be addressed in this section are also implemented in the PandoraFastReco and PandoraCosmic reconstruction paths: this highlights the true power of Pandora-based reconstruction pipeline, which allows for a large degree of flexibility. 

\subsection{Two-dimensional clustering}

The first algorithms in PandoraNeutrino reconstruction path aim at clustering the hits on each of the three readout planes. The TrackClusterCreation algorithm is involved in this step in the ``nominal'' Pandora reconstruction (i.e., without any cheating applied). It performs the clustering of adjacent hits representing continuous lines, operating only on topological features. Cheating this step is straightforward. Using simulated data, the hits on the three views, $u$, $v$ and $w$, are associated with a value called ``MCWeight'', expressing the contribution of a given Monte Carlo particle in the interaction to the simulated hit on the readout plane. Using the MCWeights, it is possible to map the hits to the respective MC particle and thus build a cluster on each of the 2D planes using the simulation's true information. The effect for tracks, like muons and protons, is noticeable but not striking, whereas the impact on electromagnetic showers is relevant, since this approach does not just cluster continuous lines of hits but, using MC association to the true underlying event, can group together hits in more complex topologies. Hence, the presence of the CheatingClusterCreation algorithm makes the contribution of some of the subsequent algorithms to the reconstruction of the event hierarchy irrelevant. 

Like most of the algorithms used within Pandora framework, the CheatingClusterCreation algorithm is highly customisable, allowing only one view of the three to be cheated and also allowing only one type of particle to have its cluster created by using true information. Due to the ICARUS geometry and the details discussed in \autoref{sec:ICARUS_T600} (illustrated in \autoref{fig:i2_c_planes_wirepitch_detail}), the first option is only possible considering either the induction-1 plane (associated with the $w$ view) and/or the induction-2 and collection plane together (associated with a mix of $u$ and $v$ views).

\autoref{fig:CheatingClusterCreation} illustrates the action of the cheated algorithm, showing the clusters created after a single pass of the TrackClusterCreation algorithm and the cluster created by the CheatingClusterCreation algorithm. The effect, though very subtle, is noticeable, especially in the cluster refinements. A smaller fraction of noise hits are clustered together with particle hits, resulting in an overall improvement in the downstream reconstruction of the 3D particles. 

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth, trim={12cm 0 11cm 0}, clip]{pandora/chapter_4/cluster2D.pdf}
    \caption[CheatingClusterCreation versus TrackClusterCreation algorithm]{Illustration of the effect of the CheatingClusterCreation algorithm, with a $1\PGm1\Pp$ event. The reconstruction with the cheated cluster creation (shown in the last panel) is closer to the true underlying event (shown in the first panel). Also, compared to the ``nominal'' reconstruction (shown in the middle panel), the reconstructed interaction appears to have fewer noise hits associated and a more refined set of clusters. }
    \label{fig:CheatingClusterCreation}
\end{figure}

To validate and assess the impact of the CheatingClusterCreation algorithm, we need to define a downstream reconstruction metric. Given that the TrackClusterCreation algorithm objective is to assign the correct hits to the respective track in each plane, two valid metrics are the \emph{hit completeness} and \emph{hit purity} scores. Given the hits on the readout plane, it is possible to define the MC matched hits, as the hits that are associated with the true MC particle and are also associated by Pandora to the reconstructed so-called PFParticle \begin{equation}
    \mathrm{Matched\ hits} \equiv \mathrm{hits_{MC particle} \cap hits_{PFParticle}}.
\end{equation} \autoref{fig:hit_pur_eff} illustrates this concept on an event with two particles. The reconstructed PFParticle $j$ has a total of seven hits associated with Pandora reconstruction, whereas PFParticle $k$ has six; the true MC particle $j$ has nine hits and $k$ has four. So for Particle $j$ the matched hits are seven, and for particle $k$ the number is four. If we introduce the definition of hit purity and hit completeness as \begin{equation}
    \mathrm{Hit\ purity} \equiv \frac{\mathrm{Matched\ hits}}{\mathrm{hits_{PFParticle}}}, 
\end{equation} and \begin{equation}
    \mathrm{Hit\ completeness} \equiv \frac{\mathrm{Matched\ hits}}{\mathrm{hits_{MC particle}}}. 
\end{equation} we can say that for the case shown in \autoref{fig:hit_pur_eff} we have a purity of \SI{100}{\percent} and \SI{66.7}{\percent}, and a completeness of \SI{77.8}{\percent} and \SI{100}{\percent}, for the $j$ and $k$ particle, respectively. 

\begin{figure}
    \centering
    % \subfloat[]{
    \begin{tikzpicture}
        \node[] at (0,0) {\includegraphics[width=0.5\linewidth, trim={18cm 0 18cm 0}, clip]{pandora/chapter_4/HIT_PUR_EFF.pdf}};
        \node[] at (6, 0) {
        \begin{minipage}[b]{5cm}
            \begin{tabular}{ccc}
                & Particle $j$ & Particle $k$ \\
                $\mathrm{hits_{MC particle}}$ & 9 & 4 \\
                $\mathrm{hits_{PFParticle}}$ & 7 & 6 \\
                Matched hits & 7 & 4 \\
                Purity & 1 & 0.667 \\
                Completeness & 0.778 & 1
            \end{tabular}
        \end{minipage}
        };
    \end{tikzpicture}
    
    \caption[Definition of hit purity and completeness]{Illustration of the process of hit matching to truth information on an event with two true and two resulting reconstructed particles. More details are provided in the text. }
    \label{fig:hit_pur_eff}
\end{figure}

Given the definitions of hit purity and completeness, we can use them to assess the impact of the CheatingClusterCreation on the event reconstruction. \autoref{fig:hit_purity_completeness_CheatingClusterCreation} illustrates both the hit purity and the hit completeness for a sample of $1\PGm N\Pp$ events selected using the true signal definition \cite{artero_pons_2024_13841852}, for both protons and muons involved in the process. Both hit purity and completeness spectra highlight an improvement in the case of the cheating compared to the nominal 2D clustering algorithm.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{pandora/chapter_4/toSlide_completeness_purity.pdf}
    \caption[Hit purity and completeness with CheatingClusterCreation algorithm]{Hit purity and hit completeness spectra for the protons (blue) and muon (red) population, for both the cheated cluster reconstruction (thick line) and the nominal reconstruction (thin line).}
    \label{fig:hit_purity_completeness_CheatingClusterCreation}
\end{figure}

However, there is a non-zero fraction of events for which the muon completeness is lower than \num{0.4}. 

Visually scanning these events unveiled that two main issues arise: \begin{enumerate}
    \item Particles have poorly reconstructed hits on some of the wireplanes, making the correct association between views in downstream algorithms tough event if cheating the 2D clustering removes ambiguities at this level;
    \item In some cases, the upstream slice creation fails and splits the muon into two or more slices. The slice that is reconstructed as a $1\PGm N\Pp$ interaction and matches the truly generated one in these cases contains a muon with a smaller than true track length, resulting in a higher hit purity compared to the remaining slices but smaller than one hit completeness.
\end{enumerate} This visual scanning reveals that it is very difficult to decouple the effects of the algorithms upstream in the event reconstruction from the algorithms downstream; since the hit purity and hit completeness metrics are computed downstream of the event reconstruction, these take into account all these effects and are not uniquely affected by the clustering performance. For instance, after the two-dimensional clusters are created, clusters are modified iteratively by the Overshoot- and UndershootTracksTool algorithms that help perform the 3D reconstruction. Similar clustering improvements are performed also before three-dimensional reconstruction is done, once the interaction vertex is created, and after 3D particle creation when the particle refinement tools are run. These downstream operations can cause a cluster to be split into multiple smaller clusters, thus creating multiple reconstructed particles that are matched to the same true particle, thereby undermining the impact of the 2D clustering cheating on the entire event reconstruction. By selecting all the particles that are truth-matched to a muon in the interaction, one recovers the entire muon track. So, it is evident that these events, showing a smaller hit completeness for both protons and muons, are not directly a result of an issue of the CheatingClusterCreation algorithm but more related to issues upstream (i.e., signal processing on the readout planes) or downstream (i.e., 3D reconstruction) of the CheatingClusterCreation algorithms. 
Comparing the hit completeness with the number of reconstructed hits on the collection plane, left and right plots in \autoref{fig:CompletenessVsHits}, it is possible to double-check that low hit completeness values are related to a small number of reconstructed hits. 

\begin{figure}
    \centering
    \subfloat[]{\includegraphics[width=0.5\linewidth]{pandora/chapter_4/muonCompletenessVsHits_cheated2d.pdf}\label{fig:muonCompletenessVsHits_cheated2d}}
    \subfloat[]{\includegraphics[width=0.5\linewidth]{pandora/chapter_4/protonCompletenessVsHits_cheated2d.pdf}\label{fig:protonCompletenessVsHits_cheated2d}}
    \caption[Hit completeness versus number of hits on collection plane]{Number of hits reconstructed on the collection plane  and associated with a reconstructed particle versus the hit completeness of the same reconstructed particle. The left plot shows the reconstructed particles associated with true muons, whereas the right one shows the same quantities for the protons. Both plots are generated for the case of performing the cheating of the cluster creation. }
    \label{fig:CompletenessVsHits}
\end{figure}

\autoref{fig:lowCompleteness_sliceErr} shows a case in which reconstruction issues are due to failure in the slice creation step, resulting in a less than 0.5 hit completeness for the reconstructed muon.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\linewidth, trim={17cm 0 17cm 0}, clip]{pandora/chapter_4/lowCompleteness_sliceErr.pdf}
    \caption{Example of an event for which the reconstructed muon completeness is lower than $0.4$, i.e., $\PGm_\mathrm{completeness} \simeq \num{0.318}$. In this case the lower completeness is due to the true muon particle being split into two slices, of which only one (slice A) is reconstructed as corresponding to a $1\PGm5\Pp$ interaction and thus selected as a signal candidate. }
    \label{fig:lowCompleteness_sliceErr}
\end{figure}

\subsection{Three-dimensional vertex} \label{sec:cheating3dVertex}

Recalling the description of the vertex reconstruction provided in \autoref{sec:TPC_reco_gen}, two algorithms are mostly involved. The first, the CandidateVertexCreation algorithm, performs the creation of two candidate vertices for each 2D cluster created, comparing pairs of clusters from two different readout planes, therefore providing a list of all the ``candidate vertices'' in the interaction. The second, the BdtVertexSelection algorithm, selects from the list of candidate vertices the most probable interaction vertex, using a BDT algorithm and some geometric variables extracted from the clusters and the candidate vertices. 

There are two ways true information can be used to inform the reconstruction of the interaction vertex. 
The most intuitive cheating mode is represented by the replacement of both steps in the vertex identification (candidates creation and final selection) with the assignment of the true 3D position of the interaction vertex from Monte Carlo simulation. This operation is performed by the CheatingVertexCreation algorithm that replaces all the vertex creation and selection algorithms in the XML steering configuration. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\linewidth, trim={12cm 0 11cm 0}, clip]{pandora/chapter_4/vertex.pdf}
    \caption[CheatingVertexCreation and CheatingVertexSelection algorithms]{Illustration of the impact on Pandora reconstruction of the CheatedVertexCreationAlgorithm. On the left panel, showing the nominal vertex reconstruction, the true neutrino interaction vertex is shown by a dotted cross $+$, whereas the filled circle ($\circ$) indicates Pandora-reconstructed neutrino interaction vertex. In this case, vertex identification fails, resulting in a (3D) distance between true and reconstructed vertex of half a metre. In the right panel, the outcome of cheating vertex identification is shown, and, as expected, the filled circle matches the true interaction vertex. Additionally, the vertex selected by the CheatingVertexSelection algorithm is shown with a dashed circle. }
    % This is a $1\PGm1\Pp$ event where the primary proton (short track) and the muon (long track) are produced nearly back-to-back. The muon decay producing a Michel electron. 
    \label{fig:CheatingVertexCreation}
\end{figure}

\autoref{fig:CheatingVertexCreation} shows an example of an event where the nominal reconstruction misplaces the interaction vertex. The event is a $1\PGm1\Pp$ event where the primary proton (short track) and the muon (long track) are produced nearly back-to-back. The muon decays in argon, thereby generating a Michel electron. Due to the topological features of the interaction, the vertex in the nominal reconstruction is placed at a distance of ${\sim}\SI{50}{\cm}$ from the true position. Using the CheatedVertexCreation algorithm, the correct interaction vertex is assigned, as shown in the right panel of \autoref{fig:CheatingVertexCreation} where the true and reconstructed vertices coincide.
It is worth noting that the impact of correct vertex reconstruction on the following steps of particle cluster creation is drastic: In the example shown in \autoref{fig:CheatingVertexCreation}, the proton (the particle on the left side of the vertex) is not reconstructed in the nominal case while correctly identified when the vertex reconstruction is cheated.

However, there are some caveats to take into account when evaluating vertex reconstruction performance. Since Pandora operates its algorithms on the reconstructed and filtered hits, it can only add vertices at the endpoint of 2D clusters. However, there are cases where the true interaction vertex position does not correspond to any reconstructed hits. This can happen in these three noteworthy cases: \begin{enumerate}
    \item the interaction vertex lies outside the active volume of the TPC, therefore not producing any signal in the wires in its vicinity;
    \item the conversion gap (i.e., the distance of the first visible hit from the interaction vertex) for the final state particles involved in the interaction is large due to the underlying physics (for example, the production of a $\PGpz$, that is detected through the identification of two $\PGg$ yielding electromagnetic showers in LAr);
    \item the hits near the true interaction vertex are lost in the signal processing stage that takes place before Pandora performs hierarchy reconstruction upon the event. 
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.55\linewidth, trim={18.5cm 0 22cm 0}, clip]{pandora/chapter_4/vertex_OoFV.pdf}
    \caption[CheatingVertexCreation with an OoFV vertex]{The illustration shows an example of events where the simulated vertex lies and the outcome of cheating vertex reconstruction. As discussed in detail in the text and expected, the position of the reconstructed vertex does not match the true one. }
    \label{fig:CheatingVertexCreation_errors}
\end{figure}

In all these cases, cheating the vertex with the CheatingVertexCreation algorithm results in a vertex placement far from reconstructed hits on the three views. Pandora algorithms downstream of the vertex creation operate on the events and partially solve any ambiguity left by assigning the vertex to the closest hit in 3D space. \autoref{fig:CheatingVertexCreation_errors} shows one of the mentioned cases. In this example, the simulated true vertex is outside the fiducial volume (hence out of the active TPC volume), and the candidate vertex reconstructed and identified by Pandora is highlighted yellow, far from the true position. It is worth remarking that events showing such problems (called \emph{dirt events}) are excluded from the sample of events used in this work, by requiring the vertex to be contained in the fiducial region of the TPC active volume; therefore, such problems do not arise in future results. 

A second point where the true Monte Carlo information can be used to inform the vertex algorithms, and thus a second possible cheating mode of the vertex identification, is the selection of the correct interaction vertex from the list of candidates created by the nominal CandidateVertexCreation algorithm. This operation essentially entails bypassing the BDT performing the choice and selecting, from the list of vertex candidates created as described in \autoref{sec:PandoraNeutrino}, the vertex which lies closest to the true interaction vertex. \autoref{fig:CheatingVertexCreation}, on the right panel, shows the result of cheating the vertex selection (dashed circle). 

Validating the cheating of the vertex reconstruction is straightforward: it is possible to check the distance of the true vertex with respect to the reconstructed vertex of the interaction. \autoref{fig:vertex_cheated_dispacement} shows this for both the cheating of the vertex creation (thick black line) as well as the cheating of the vertex selection (thin black line), comparing the two cases with the distribution of the vertex distance when the nominal reconstruction is performed. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{pandora/chapter_4/toSlide_vertexStudy.pdf}
    \caption[Vertex displacement from truth]{Displacement of the reconstructed vertex from the true vertex, evaluated in three cases: with the nominal vertex reconstruction (shaded area), cheating the vertex selection (thin black line) and cheating the vertex creation algorithm (thick black line). }
    \label{fig:vertex_cheated_dispacement}
\end{figure}


\subsection{Three-dimensional track and shower reconstruction}

The reconstruction of the 3D particles, starting from the 2D clusters created by upstream algorithms, as described in the subsections of \autoref{sec:TPC_reco_gen}, is not trivial. It encompasses a large number of algorithms and tools: the starting point is the refinement of the two-dimensional clusters on different readout planes to remove any ambiguities (hits that have common $x$ drift coordinate, that are from clusters of different particles) in order to create 3D clusters across the readout planes. Using the information from the different readout planes, together with the wire orientation, the 3D positions are computed, and the 2D clusters are projected into three-dimensional space. 

Cheating the 3D reconstruction has a simpler structure: first, 2D clusters are associated across the readout planes by the CheatingPfoCreation algorithm; then, the ThreeDHitCreation algorithm performs the reconstruction of the 3D hits. This second step is the same as the nominal reconstruction: even though the interaction is generated in three-dimensional space, the simulation of the hits is performed only on the 2D readout planes, so there is no possibility to really inject the true $(x,y,z)$ position of each hit, since it is not known a priori.

The CheatingPfoCreation algorithm goes through all the 2D clusters created by upstream stages of the reconstruction to identify the ones that share the same true MC particle. This operation is performed, similarly to the CheatingClusterCreation algorithm, exploiting the Monte Carlo weights associated with the hits in the cluster. For each MC particle in the interaction, the algorithm uses the MC weights to map the underlying particle to the deposited hits on the readout plane and then to the corresponding clusters on the different planes. Only clusters sharing more than \SI{50}{\percent} of the hits with the underlying particle are associated with the correct particle; the other clusters (those that share less than \SI{50}{\percent} of the hits with the MC particle) are discarded for three-dimensional reconstruction. From these 3D clusters the 3D hits are reconstructed from geometrical considerations, as the nominal reconstruction. 

It is worth noting that the CheatingPfoCreation algorithm does not alter in any way the output of the previous stages (unlike the nominal reconstruction, which implements the Under-/OverShootTracksTools to refine 2D clusters). Therefore, any issue of the reconstruction upstream of this stage is inherited and does impact the performance of this reconstruction. 

In cases where a plane contains two ambiguous clusters, e.g., as a consequence of 2D clustering algorithms splitting in two parts a truly single cluster, only the cluster sharing the largest fraction of the hits with the true MC particle will be considered for the 3D reconstruction, thereby affecting the overall reconstruction efficiency and hit completeness for the particles involved.
This is why the cheating of this step of the reconstruction is only performed cumulatively, and not as an individual module, as mixing nominal and cheating at this level might cause unrealistic reconstruction pathologies and thus bias the evaluation of the performance of these reconstruction steps. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{pandora/chapter_4/toSlide_completeness_purity_3d.pdf}
    \caption[Hit purity and completeness with CheatingPfoCreation algorithm]{Hit purity and hit completeness spectra for the proton (blue) and muon (red) population when the nominal reconstruction is applied (thin line), when the 2D clusters are cheated (dashed line) and when all the steps up to the 3D cluster matching are cheated (thick line).}
    \label{fig:hit_purity_completeness_CheatingPfoCreation}
\end{figure}

To validate the effectiveness of cheating the three-dimensional cluster matching, the same metric used to evaluate the effectiveness of 2D cluster creation algorithms is used. This is illustrated in \autoref{fig:hit_purity_completeness_CheatingPfoCreation}, where an improvement to the bare cheating of the 2d cluster creation algorithm (shown in the dashed line) is visible. 

\subsection{Particle hierarchy reconstruction} 

The particle hierarchy reconstruction, implemented by three algorithms in the nominal Pandora event reconstruction, performs first the association between the primary particles and the interaction vertex. Then any leftover secondary particle is associated with its parent, and the vertex is computed using the distance of the daughter from the parent. Finally, all the daughters are associated with their parents, and the full particle hierarchy is saved.

The cheating of the particle hierarchy reconstruction, implemented by the CheatingNeutrinoCreation and CheatingNeutrinoDaughterVertices algorithms, aims at assigning the correct parent-daughter relationship to all reconstructed particles in the interaction. Using MC weights, it associates a reconstructed particle with the underlying MC particle that has the highest level of compatibility. Using this association, it then assigns the reconstructed neutrino interaction vertex to the particles identified as primary from the underlying MC particle. The same operation is then performed for secondary particles in the interaction: daughter particles are associated with their parents, and secondary vertices are created. 

Since this step takes place downstream of the reconstruction chain, its performance is highly impacted by the outcome of previous stages, and extracting a reliable estimate of its effectiveness is not an easy task. As for previous algorithms, its capability to provide a reliable result depends on the upstream reconstruction stages, since the MC particle-reconstructed particle association is performed using the MC weights: if a MC particle gets split into two or more reconstructed particles, only the particle sharing the largest fraction of hits will be assigned the correct parent-daughter hierarchy.

Ideally, assigning the correct parent-daughter matches would result in more primary particles identified with this label. Also, a greater purity of the events selected should be observed: no contamination from reinteracting particles that are not individually reconstructed should be present. Thus, for the $\PGnGm$CC QE Np analysis, cheating the particle hierarchy should result in the proton multiplicity in the event being assigned correctly. \autoref{fig:Np} shows the comparison of reconstructed and true proton multiplicity, highlighting that if the reconstruction is cheated all the way up to the particle hierarchy creation, as shown in \autoref{fig:Np}\ref{sub@fig:Np_cheated_2d_vtx_3d_nu}, the diagonal is more prominent with respect to the nominal reconstruction, shown in \autoref{fig:Np}\ref{sub@fig:Np_nominal}. 

\begin{figure}[!htb]
    \centering
    \subfloat[]{\includegraphics[width=\linewidth]{pandora/chapter_4/Np_comparison_nominal.pdf}\label{fig:Np_nominal}}
    
    \subfloat[]{\includegraphics[width=\linewidth]{pandora/chapter_4/Np_comparison_cheated_2d_vtx_3d_nu.pdf}\label{fig:Np_cheated_2d_vtx_3d_nu}}
    \caption[True versus reconstructed primary proton multiplicity]{Reconstructed versus true proton multiplicity distribution. All the numbers are normalised with respect to the true number of protons in the interaction, i.e., normalised on a column basis. These plots show that when all the steps of the reconstruction up to the particle hierarchy creation are cheated, more events are assigned the correct number of primary protons. }
    \label{fig:Np}
\end{figure}

This step is, however, delicate; hence, some remarks are mandatory to give a correct interpretation to the action of cheating in this specific case. Firstly, although correctly assigning the particle hierarchy is crucial for the downstream event selection, it is not the only factor that affects it, as the multiplicity of the reconstructed protons highly depends on the efficiency of the particle identification. Furthermore, in \autoref{fig:Np}\ref{sub@fig:Np_cheated_2d_vtx_3d_nu} there are some cases, especially when the proton multiplicity is higher, where the reconstruction fails to identify the associated hits. 
This is due to the fact that, for a certain interaction energy that is fixed to the value of the incoming neutrino (peaked at \SI{800}{\mega\electronvolt}), a larger number of protons in the final state will imply a smaller amount of energy for each of them and thus a more complex topology to disentangle from noise. This in turn will make hit reconstruction on the wireplanes more difficult. Finally, given the event selection cuts presented in \autoref{sec:dataSample_and_selection}, two aspects are noteworthy: \begin{enumerate}
    \item The present selection strongly relies on this step, since it requires both the muon and the protons identified within the interaction to be primary particles, as well as ensuring that any other particle identified as belonging to a different species is not primary in the interaction. 
    \item Since a proper separation of secondary particles from primaries is known to be delicate in Pandora reconstruction and having secondaries collapsed onto primary particles is a common failure mode, the event selection for the present $\PGnGm$ oscillation analysis was tuned accordingly. This is why in the event selection the requirement for the primary protons to have at least \SI{50}{\mega\electronvolt} of deposited energy was extended, at the truth level, to include also all their daughter particles, since in the reconstructed final states such behavior is expected. 
\end{enumerate} 

These conditions, especially the latter, basically imply that the event selection does not rely on the creation of the particle hierarchy. Therefore, altering this step by injecting the true MC information to improve its performance,  does not necessarily have a proportional impact on the reconstruction and selection efficiency. In reality, the interplay between how the nominal algorithm and its cheated version work causes some events that are selected with the nominal particle hierarchy creation algorithm to be lost due to misidentification in downstream event selection. 


The issues preventing the correct reconstruction of the particle hierarchy and the correct identification of the interaction in some cases are multiple. Here we highlight two of the most common cases: \begin{itemize}
    \item There are cases where all the particles in the final state are reconstructed, but where the prompt protons do not fullfill the selection requirements. Take for example \autoref{fig:particleHierarchy}\ref{sub@fig:particleHierarchy_lowEProton}. In this case, the truth say that the prompt proton and its daughter particles add up to a deposited energy $E_\mathrm{dep}>\SI{50}{\MeV}$ (therefore a proton longer than \SI{2.3}{\cm}). However, if the reconstruction is cheated up to the particle hierarchy creation, the reconstructed prompt proton (alone, without its daughters) is shorter than \SI{2.3}{\cm} (or equivalently deposits less than \SI{50}{\MeV}. Therefore, even if the event is a signal event, it is not selected, lowering the efficiency. However, such events are selected when performing the cheating up to the stage that performs the three-dimensional reconstruction, since Pandora (mis-)identifies the daughter proton of the prompt proton reinteraction as a primary particle.
    
    \item Another case is represented by events where the prompt proton produces visible hits only on one readout plane. This happens e.g. for very short prompt protons that reinteract in LAr, producing a daughter proton. \autoref{fig:particleHierarchy}\ref{sub@fig:particleHierarchy_missingHits} illustrates this failure mode with an example. In this case, the prompt proton is not reconstructed (as Pandora requires at least two planes to perform a three-dimensional reconstruction). If the particle hierarchy creation is left ``uncheated'', its daughter particle gets assigned as a primary particle of the interaction vertex. When the particle hierarchy is cheated, the correct hierarchy is assigned, and this results in an event with no primary proton ($1\PGm0\Pp$) that is rejected by the selection.
\end{itemize}

\begin{sidewaysfigure}
    \centering
    \subfloat[]{\includegraphics[height=8cm, trim={10cm 0 22cm 0}, page=2]{pandora/chapter_4/particle_hierarchy_error.pdf}\label{fig:particleHierarchy_lowEProton}}
    \subfloat[]{\includegraphics[height=8cm, trim={22cm 0 8cm 0}, page=1]{pandora/chapter_4/particle_hierarchy_error.pdf}\label{fig:particleHierarchy_missingHits}}
    \caption[Particle hierarchy cheating failure modes]{\ref{sub@fig:particleHierarchy_lowEProton} illustrates one event where the primary (prompt) proton is shorter than the lower threshold of \SI{2.3}{\cm} or equivalently \SI{50}{\MeV} of deposited energy. This event is therefore not selected if the daughter proton of the prompt proton is assigned the correct parent-daughter hierarchy, like when performing the cheating of the particle hierarchy. \ref{sub@fig:particleHierarchy_missingHits} illustrates the case where the prompt proton is not reconstructed due to missing information on some of the readout planes. This event is not selected since the secondary proton, daughter to the prompt proton, is not identified as primary when the particle hierarchy is cheated. }
    \label{fig:particleHierarchy}
\end{sidewaysfigure}

After specific tests to better understand how particle hierarchy algorithms and their cheated version work, and considering the pathological cases partially due to the event selection used for the present analysis, we decided to keep the nominal version of this stage in the reconstruction chain.

\subsection{Particle classification}

The last step of Pandora neutrino reconstruction path, aimed at classifying particles into tracks or electromagnetic showers based solely on topological and calorimetric features, can be cheated in two different ways. 

The easiest way entails simply referring to the Particle Data Group (PDG) label, uniquely identifying each MC particle with a label and assigning it correctly to the reconstructed particle. This is the strategy that was originally implemented as part of Pandora cheated reconstruction chain but is not useful for the present work, since the event selection for the $\PGnGm$CC QE Np ICARUS analysis does not rely on this information but instead makes use of the track/shower classification BDT score that is produced by the nominal reconstruction algorithm. So another way of cheating this stage of the reconstruction was implemented as part of this work. This implementation uses the true PDG label associated with MC particles to cheat the value of the BDT track score, effectively making the track-shower separation perfect. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\linewidth]{pandora/chapter_4/toSlide_BDT_trackscore.pdf}
    \caption[Track and shower classification BDT score]{Plot of the score resulting as output of the track and shower BDT classification algorithm. Track-like particles should have a score of 1, whereas electromagnetic shower particles should have a score of 0. In the two panels we show muons (upper half, in red) and protons (lower half, in blue) with both the nominal reconstruction (thin line) as well as the cheated reconstruction and classification (thick line). }
    \label{fig:trackScoreCheated}
\end{figure}

Checking this stage is straightforward since one can simply rely on true Monte Carlo label for the reconstructed particle cluster to assign with no ambiguities the correct track score: 0 for shower-like particles such as photons and 1 for track-like particles such as protons and muons. \autoref{fig:trackScoreCheated} shows the assigned BDT score with the nominal (thin line) and cheated (thick line) particle classification stage for protons (blue) and muons (red), where the latter cheated version was adopted. 

\section[Efficiency evaluation for different reconstruction stages]{Efficiency evaluation for different reconstruction stages: approach and results}\label{sec:methods}

% aim of performing this analysis
Using this set of tools, whose performance have been thoroughly evaluated and whose limitations have been highlighted in the previous paragraphs, it is now possible to get a detailed understanding of the most critical reconstruction stages and understand the source of errors and how to address them, weighing their relevance on the impact these changes can have based on the reconstruction and downstream event selection performance. 

Dealing with this task, we first focused on PandoraNeutrino reconstruction path. We can separate the operations of this stage into five substages: the 2D cluster creation, the 3D vertex creation, the 3D particle creation, the particle hierarchy creation, and the particle classification. As already stated previously, we will exclude the particle hierarchy identification from the present study.


Given the present division in subsequent reconstruction steps, we can validate their performance by comparing the cheated and uncheated versions. The idea is to compare five reconstruction chains that process the same events. Each one differs from the previous in that a stage previously cheated to reach maximal efficiency is now replaced by its standard (i.e., uncheated) version. Starting from a fully cheated configuration, where the 2D cluster creation, the 3D vertex creation, the 3D particle creation, and the particle classification are cheated, we then replace the last step with its nominal/standard uncheated version, leaving only the three steps happening before as cheated. Continuing on with the same idea for all the stages, with the exception of the particle hierarchy creation algorithm,  the last explored configuration has all the stages in their nominal format. Schematically, representing with a gray square each substage of the reconstruction ({\nominal}), and filling it red when it is cheated ({\cheated}), the configurations reported in \autoref{tab:configurations} are shown below (here the columns are the individual substages of the reconstruction chain)\begin{center}
    \begin{tikzpicture}[scale=0.23]
        \foreach \stage [count=\i] in {Track score, 2D $\to$ 3D, Vertex creation, 2D clustering} {
            \node[anchor=west, rotate=30] at (-{\i*2.5}, 1) {\footnotesize\stage};
            \foreach \letter [count=\j] in {A,...,E} {
                \node[anchor=east] at (-11,-{(\j-0.4)*1.1}) {
                    \footnotesize 
                    \ifnum \j < 2
                        Config. \letter
                    \else 
                        \dots\ \letter
                    \fi
                };
                \ifnum \j < \i
                    \draw[fill=Red, line width=0.5, line cap=round] 
                        (-{\i*2.5},-{\j*1.1}) rectangle ++(1.75, .8);
                \fi

                \ifnum \j > \i
                    \draw[fill=Gray, line width=0.5, line cap=round] 
                        (-{\i*2.5},-{\j*1.1}) rectangle ++(1.75, .8);
                \fi

                \ifnum \j = \i
                    \draw[fill=Red, line width=0.5, line cap=round] 
                        (-{\i*2.5},-{\j*1.1}) rectangle ++(1.75, .8);
                \fi
            }
        }
        
        
    \end{tikzpicture}
\end{center}

% method
Under the assumption that Pandora reconstruction algorithms are decoupled from one another (see \autoref{sec:TPC_reco_gen} and references therein), it is possible to state that the overall event reconstruction and selection efficiency is given by the product of the efficiency of the signal processing stage (see \autoref{sec:TPCSignalReconstruction}) and the efficiencies of the individual steps of the event reconstruction and the event selection efficiency, i.e., \begin{equation}
    \begin{aligned}
        \epsilon &= 
        \epsilon_\mathrm{signal\ processing} \times 
        \epsilon_\mathrm{2D\ clustering} \times 
        \epsilon_\mathrm{vertex\ creation} \times \\
        &\quad\quad\quad\times
        \epsilon_\mathrm{3D\ reconstruction} \times 
        \epsilon_\mathrm{particle\ classification} \times 
        \epsilon_\mathrm{event\ selection}
    \end{aligned}\label{eq:componentsEfficiency}
\end{equation} 

Making the logical assumption that cheating a step of the reconstruction makes its efficiency effectively \SI{100}{\percent}, and indicating with an asterisk the efficiency of a cheated reconstruction stage, it is possible to compare two of the aforementioned configurations that differ only  in how a single step operates. For example, the ratio between the efficiencies of the ``fully cheated'' configuration (A) and that with all stages cheated except for the last (configuration B) is \begin{equation}
    \frac{
    \epsilon^\mathrm{B}
    }{
    \epsilon^\mathrm{A}
    } = \frac{
    \epsilon_\mathrm{sig.} \times 
    \epsilon_\mathrm{2D}^* \times 
    \epsilon_\mathrm{vertex}^* \times 
    \epsilon_\mathrm{3D}^* \times 
    \epsilon_\mathrm{class.} \times 
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{B}
    }{
    \epsilon_\mathrm{sig.} \times 
    \epsilon_\mathrm{2D}^* \times 
    \epsilon_\mathrm{vertex}^* \times 
    \epsilon_\mathrm{3D}^* \times 
    \epsilon_\mathrm{class.}^* \times 
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{A}
    } = \frac{
    \epsilon_\mathrm{class.}
    }{
    \epsilon_\mathrm{class.}^*
    } \times \qty(\frac{
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{B}
    }{
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{A}
    }) \label{eq:methodBase}
\end{equation} Here the apex refers to which of the configurations is used, and the asterisk is used to distinguish the cheated from the nominal version of a given reconstruction stage. All the configurations are listed in \autoref{tab:configurations}, and the same notation is adopted for all the equations/for all the counts from now on. Additionally, the notation of the single stages is simplified for the sake of brevity. 

\begin{table}[]
    \centering
    \caption[List of configurations]{List of all the configurations considered for the evaluation of the individual and global reconstruction performance in \autoref{sec:methods}. The red cross mark {\tikzxmark} indicates the steps of the reconstruction that are kept nominal, whereas the green tick mark {\tikzcmark} indicates the ones that are cheated. }
    \label{tab:configurations}
    \small
    \begin{tabular}{lp{3.5cm}cccc}
        \hline
         & & 2D clusters & Vertex & 3D particles & Particles \\
         Id. & Configuration name & creation & creation & reconstruction & classification \\
         \hline
         A & Fully cheated & \tikzcmark & \tikzcmark & \tikzcmark & \tikzcmark \\
         B & Cheated up to the particle classification algorithm & \tikzcmark & \tikzcmark & \tikzcmark & \tikzxmark \\
         C & Cheated up to the particle three-dimensional reconstruction algorithm & \tikzcmark & \tikzcmark & \tikzxmark & \tikzxmark \\
         D & Cheated up to the vertex reconstruction & \tikzcmark & \tikzxmark & \tikzxmark & \tikzxmark \\
         E & Nominal & \tikzxmark & \tikzxmark & \tikzxmark & \tikzxmark \\
         \hline
    \end{tabular}
\end{table}

It is possible to redraft/rewrite Eq. \eqref{eq:methodBase} as follows, \begin{equation}
    \epsilon_\mathrm{class.} \times \qty(\frac{
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{B}
    }{
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{A}
    }) = \frac{
    \epsilon^\mathrm{B}
    }{
    \epsilon^\mathrm{A}
    } \times \epsilon_\mathrm{class.}^* \label{eq:stageEfficiencyPrePID_0}
\end{equation} It is worth noting that, under the reasoned assumption that $\epsilon^* = \SI{100}{\percent}$ for any cheated stage, we can drop this term in Eq. \eqref{eq:stageEfficiencyPrePID_0} to obtain \begin{equation}
    \epsilon_\mathrm{class.} \times \qty(\frac{
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{B}
    }{
    \epsilon_\mathrm{ev.\ sel.}^\mathrm{A}
    }) = \frac{
    \epsilon^\mathrm{B}
    }{
    \epsilon^\mathrm{A}
    } \label{eq:stageEfficiencyPrePID}
\end{equation} It is worth remarking that this result is exactly the same as comparing the number of selected signal candidates in the two configurations, $\epsilon^\mathrm{B}/\epsilon^\mathrm{A}\equiv N_\mathrm{signal}^\mathrm{B}/N_\mathrm{signal}^\mathrm{A}$. 

\subsection{Results} \label{sec:resultsLadder}

%% evaluation of the erformances
%% step by step issues (if any)
Once an analysis method is established, we can start our evaluation by computing the reconstruction and selection efficiency for all the configurations shown in \autoref{tab:configurations}. The first step was to compute the integral efficiency by extracting the ratio between the number of true and selected $\PGnGm$CCQE Np events and the number of true $1\PGm N \Pp$ interactions. These efficiencies are compared in \autoref{fig:efficiencyNoNu}. A first remark is that, as more steps of the reconstruction are cheated, the overall efficiency increases: this suggests that, at least, the addition of any cheated algorithm improves the event reconstruction, which was expected but not necessarily granted. Further observations follow from the visual inspection of the efficiencies achieved under different reconstruction configurations. For example, it is clear that once the 2D clusters are created correctly, i.e., cheated, the reconstruction performs better overall by comparing the nominal reconstruction (configuration E) with the ``cheated up to the vertex reconstruction'' reconstruction (configuration D) bin, \SI{54.7(5)}{\percent} and \SI{63.1(5)}{\percent} respectively. From this it is evident that the impact of the cluster creation algorithms on the overall outcome of the chain is high. The same holds for the difference between the ``cheated up to the vertex reconstruction'' (configuration D) and the ``cheated up to the particle three-dimensional reconstruction'' (configuration C) bins, \SI{63.1(5)}{\percent} and \SI{68.1(5)}{\percent} respectively: the vertex creation algorithm has a large impact on the outcome of the chain. The uncertainties on the efficiencies presented here are computed considering a binomial statistic for the ratio of the number of selected signal events over the total number of signal events. The treatment for the statistical uncertainties is the same for all further computations. 

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{pandora/chapter_4/CCNp_efficiencyNoNu.pdf}
    \caption[Evaluation of the reconstruction and selection efficiency for different configurations]{Evaluation of the event reconstruction and selection efficiencies for the configuration presented in \autoref{tab:configurations}. Each bin is labelled according to the stage whose cheated version is added to the full reconstruction chain; in the first only the clustering creation is cheated (configuration D), and each bin adds a cheated step up to the fully cheated configuration (A) shown in last bin. }
    \label{fig:efficiencyNoNu}
\end{figure}

% results
We can now exploit these results to compute a value which is proportional to the efficiency of each reconstruction step by factorising the event selection efficiency for each of the two configurations considered in the computation, as highlighted in Eq. \eqref{eq:stageEfficiencyPrePID}. Extracting this result for all possible combination of subsequent configurations, we can extract values proportional to the single-stage efficiencies, \begin{align}
    \epsilon_\mathrm{2D\ clusters} \times 
    \qty(\frac{\epsilon_\mathrm{ev.\ sel.}^\mathrm{E}}{\epsilon_\mathrm{ev.\ sel.}^\mathrm{D}}) 
    &= \frac{\epsilon^\mathrm{E}}{\epsilon^\mathrm{D}} = \frac{\SI{54.7(5)}{\percent}}{\SI{63.1(5)}{\percent}} = 
    \SI[round-mode = uncertainty]{86.779059+-1.104354}{\percent} \label{eq:eff2dPrePid}\\
    \epsilon_\mathrm{vertex\ creation} 
    \times \qty(\frac{\epsilon_\mathrm{ev.\ sel.}^\mathrm{D}}{\epsilon_\mathrm{ev.\ sel.}^\mathrm{C}}) 
    &= \frac{\epsilon^\mathrm{D}}{\epsilon^\mathrm{C}} = \frac{\SI{63.1(5)}{\percent}}{\SI{68.1(5)}{\percent}} = 
    \SI[round-mode = uncertainty]{92.575370+-1.024352}{\percent} \\
    \epsilon_\mathrm{3d\ reconstruction} 
    \times \qty(\frac{\epsilon_\mathrm{ev.\ sel.}^\mathrm{C}}{\epsilon_\mathrm{ev.\ sel.}^\mathrm{B}}) 
    &= \frac{\epsilon^\mathrm{C}}{\epsilon^\mathrm{B}} = \frac{\SI{68.1(5)}{\percent}}{\SI{68.4(5)}{\percent}} = 
    \SI[round-mode = uncertainty]{99.526687+-1.041368}{\percent} \\
    \epsilon_\mathrm{particle\ classification} 
    \times \qty(\frac{\epsilon_\mathrm{ev.\ sel.}^\mathrm{B}}{\epsilon_\mathrm{ev.\ sel.}^\mathrm{A}}) 
    &= \frac{\epsilon^\mathrm{B}}{\epsilon^\mathrm{A}} = \frac{\SI{68.4(5)}{\percent}}{\SI{69.1(5)}{\percent}} = 
    \SI[round-mode = uncertainty]{99.039894+-1.024644}{\percent} \label{eq:effMvaPrePid}
\end{align}

Since the signal definition is the same for all configurations, we could (qualitatively) assume that no major difference should be present in the efficiency of the event selection: in such a hypothesis, the ratio between the selection efficiencies in Eqs. \eqref{eq:eff2dPrePid}--\eqref{eq:effMvaPrePid} should be approximately one.
However, a more quantitative estimation of these efficiencies is needed to provide a stronger result. 

With this target in mind, it is interesting to evaluate the interplay between cheating cumulatively to a substage of the reconstruction chain and each of the selection cut applied (described in detail in \autoref{sec:dataSample_and_selection}). To do so, the event reconstruction and selection efficiency are computed each time changing the reconstructed signal definition by cumulatively adding selection cuts. \autoref{fig:efficiencyByCut} shows the result of this cut-by-cut efficiency for all five configurations in \autoref{tab:configurations}. 

\begin{figure}
    \centering
    \includegraphics[height=9cm]{pandora/chapter_4/CCNp_efficiencyByCut_noNu_cheatingSlice.pdf}
    \caption[Event reconstruction and selection efficiency for different cuts]{Cut-by-cut event reconstruction and selection efficiency for all configurations listed in \autoref{tab:configurations}. Selection cuts are subsequently added to the ones listed above. The top table shows the efficiency of each configuration for each selection cut. It is worth remarking that the scale on the $y$ axis is not uniform in that the region from 0.9 to 1 is scaled up to allow for better visual separation of the cuts. }
    \label{fig:efficiencyByCut}
\end{figure}

% \cleardoublepage
\input{3_mainmatter/4.1_pidCheating}
% \cleardoublepage

\section{Impact of the vertex reconstruction} \label{sec:vertexResults}

% from previous ... the vertex is quite relevant
From the results shown in \autoref{sec:resultsLadder} and \autoref{sec:efficiencyPidExtraction} it is possible to deduce that assigning correctly the interaction vertex plays a central role in the event reconstruction. This is obviously true for electromagnetic showers, where the distance from the vertex, often referred to as the ``conversion gap'', is key to separating electron-induced electromagnetic showers from photon-induced ones. Since the radiation length $X_0$ in LAr is approximately \SI{14}{\cm}, having an accurate measurement of the vertex location, with a $\mathcal{O}(\si{\mm})$ precision, is core for physics analyses, especially the ones relying on $\PGne$ interactions. Furthermore, there are instances where enhancing the vertex results in an overall improvement of the reconstruction (see \autoref{sec:cheating3dVertex} and Refs. \cite{Triozzi:2025_impactNueReconstruction, Sotgia:2025_cheatingPandoraStatus}).

% since its "uncorrelated" and do not depend on the previous step success that much, cheat it alone]
Qualitative tests were conducted on a Pandora reconstruction configuration where only certain aspects of the vertex reconstruction were cheated. The results demonstrated that this step did not suffer from the other components remaining uncheated and also resulted in a significant overall improvement in the event reconstruction performance, specifically in terms of selection efficiency enhancements for the selected events. \autoref{fig:cheatedVertexByCut} shows the impact of cheating the vertex reconstruction, in terms of efficiency, considering each cut of the event selection alone. Several elements shown in \autoref{fig:cheatedVertexByCut} are  noteworthy. The bins shown here correspond to the three cheating configurations tested for the present study and also shown in \autoref{tab:configurationsVertexCheating}. As mentioned in \autoref{tab:configurationsVertexCheating}, the ``Inj. vertex selection'' configuration results from replacing the vertex selection step alone, as opposed to the full vertex identification step, i.e., replacing with the truth information the outcome of the BDT algorithm that selects the best interaction vertex from the list of the candidate vertices. 

\begin{figure}[!htb]
    \centering
    \includegraphics[height=9cm]{pandora/chapter_4/CCNp_efficiencyByCut_singles_2d_vtx_only.pdf}
    \caption[Event reconstruction and selection efficiency with the cheated vertex creation]{Cut-by-cut event reconstruction and selection efficiency for all the reconstruction configurations listed in \autoref{tab:configurationsVertexCheating}.  Each row in the table corresponds to the addition of a new cut on top of the previous ones. The top table shows the efficiency of each configuration for each selection cut. It is worth remarking that the scale on the $y$ axis is not uniform in that the region from 0.9 to 1 is scaled up to allow for better visual separation of the cuts. }
    \label{fig:cheatedVertexByCut}
\end{figure}

\begin{table}[]
    \centering
    \caption[List of configurations (vertex cheating)]{List of all the configurations used for the evaluation of the reconstruction performance in \autoref{sec:vertexResults}. The red cross mark {\tikzxmark} indicates the steps of the reconstruction for which the nominal version is used, whereas the green tick mark {\tikzcmark} indicates those that are cheated. The orange asterisk {\tikzsmark} indicates that the cheating is done partially, such as in the case of the CheatedVertexSelection algorithm. }
    \label{tab:configurationsVertexCheating}
    \small
    \begin{tabular}{lp{3.5cm}cccc}
        \hline
         & & 2D clusters & Vertex & 3D particles & Particles \\
         Id. & Configuration name & creation & creation & reconstruction & classification \\
         \hline
         A & Cheated vertex creation algorithm (Inj. vertex) & \tikzxmark & \tikzcmark & \tikzxmark & \tikzxmark \\
         B & Cheated vertex selection algorithm (Inj. vertex selection) & \tikzxmark & \tikzsmark & \tikzxmark & \tikzxmark \\
         C & Nominal & \tikzxmark & \tikzxmark & \tikzxmark & \tikzxmark \\
         \hline
    \end{tabular}
\end{table}

Looking at the result, a similar pattern to that of the previous section, specifically in \autoref{fig:efficiencyNoNu} and \ref{fig:efficiencyByCut}, emerges. This pattern suggests that enhancing the vertex reconstruction process positively impacts the overall event reconstruction performance. The efficiencies presented in \autoref{fig:cheatedVertexByCut}, particularly the efficiency achieved with all selection criteria simultaneously applied, serve as an upper bound on the potential improvement that can be realised if the vertex reconstruction were perfect. 
The two cheated configurations listed in \autoref{tab:configurationsVertexCheating} can provide further insight into the impact of different reconstruction stages.

Starting from configuration B, we studied the configuration where only the vertex selection step is cheated, meaning that the creation of the vertex candidates is still performed by the nominal reconstruction, but only the vertex closest in 3D space to the true one gets selected. 
We interpret this outcome as the maximum improvement we could achieve by a retraining of the BDT algorithm responsible for selecting the best vertex candidate. Notably, the efficiency enhancement from the nominal value ($\epsilon = \SI{54.4(6)}{\percent}$) to the outcome of this configuration, which is $\epsilon_\mathrm{inj.\ vertex\ selection} = \SI{59.6(6)}{\percent}$, is $\SI{5.1(8)}{\percent}$. We might be tempted to conclude that retraining the BDT algorithm responsible for selecting the interaction vertex is the next step to improve the efficiency of Pandora event reconstruction. However, we should first explore all the results obtained with the other configuration. 

Consequently, we conduct our analysis on configuration A of \autoref{tab:configurationsVertexCheating}, where all the algorithms involved with the reconstruction and identification of the interaction vertex are replaced by the CheatingVertexCreation algorithm, and the rest of the reconstruction chain is left unaltered (see \autoref{tab:configurationsVertexCheating}). 
From this configuration, we intend to derive an upper bound on the reconstruction capabilities, assuming that the vertex reconstruction is optimal. Additionally, we aim to accomplish this independently of the results obtained in \autoref{sec:resultsLadder}. We can exploit this result to strengthen the outcome of the work in \autoref{sec:methods}. 

In the analysis performed in \autoref{sec:methods}, we were able to factorise the efficiency related to the PID by employing a modified event selection. To maintain the independence of this analysis, an alternative method to constrain the PID selection cut efficiency is required. We examined the distribution of the PID variables, specifically the $\chi^2$-based score defined in Eq. \eqref{eq:PID}. These distributions are presented in \autoref{fig:chi2_cheatedVertex}, alongside the ratio of the spectra of the cheated configuration over the nominal configuration. Since no difference is highlighted, in accordance with expectations, we can conclude that the enhancement in efficiency is predominantly attributable to a correct vertex assignment. 

\begin{sidewaysfigure}
    \centering
    \subfloat[]{\includegraphics[width=0.5\linewidth]{pandora/chapter_4/chi2Proton_cheatedVertex.pdf}\label{fig:chi2Proton_cheatedVertex}}
    \subfloat[]{\includegraphics[width=0.5\linewidth, trim={0 -2.5cm 0 0}]{pandora/chapter_4/chi2Muon_cheatedVertex.pdf}\label{fig:chi2Muon_cheatedVertex}}
    \caption[PID scores with the cheated vertex creation]{Particle identification score distributions for protons and muons. On the left the $\chi^2$ distribution, computed under the assumption of a proton track, is shown for both protons (blue) and muons (red), for the nominal (thin line) and cheated vertex reconstruction (thick line). The right plot shows the distribution of the $\chi^2$ score under the muon track hypothesis only for the muon population. The ratio is computed between the cheated reconstruction and the nominal one and shows no significant differences.  }
    \label{fig:chi2_cheatedVertex}
\end{sidewaysfigure}

Given that the vertex impact is entirely reflected in an efficiency improvement, and no effect of the event selection PID is present, we computed the efficiency value for both the nominal configuration and for the cheated vertex reconstruction and identification (configuration A); both results are in \autoref{fig:cheatedVertexByCut}. The difference between these two configurations, $\SI{7.0(1)}{\percent}$, represents the maximum improvement achievable by perfecting the vertex reconstruction alone, for the topology considered in this study.

To further validate this result, a parallel study was performed using a different sample of events with a completely different topology: electron neutrino charge-current quasi-elastic candidates, $\PGne$CC QE. The $\PGne$CC QE analysis reported an improvement in efficiency of ${\sim}\SI{7.3}{\percent}$, which is perfectly compatible (within less than $1\sigma$) with that found by this work \SI{7.0(1)}{\percent} \cite{Triozzi:2025_impactNueReconstruction, Sotgia:2025_cheatingPandoraStatus}.

This independent study of the vertex reconstruction and identification algorithms performance yields two noteworthy outcomes. 
The first outcome arises from the analysis conducted with configuration B. This approach demonstrated that retraining the BDT algorithm, specifically selecting the correct interaction vertex among the vertex candidates, can lead to an improvement of up to a maximum of approximately $\SI{5}{\percent}$. 
The second and most significant conclusion of this independent analysis is a cross-check of the results obtained using the analysis described in \autoref{sec:resultsLadder} and presented in Eq. \eqref{eq:results}. 
In that analysis, the vertex reconstruction and identification stage was found to have an efficiency of \SI{91.9(1.6)}{\percent}, corresponding to an ``inefficiency'' of \SI{8.1(1.6)}{\percent}. 
Similarly, this independent analysis revealed that the vertex creation algorithm was found to have an ``inefficiency of \SI{7.0(1)}{\percent}. 
These two values are in good agreement, compatible within 1$\sigma$, proving two fundamental points. Firstly, that the results presented in Eq. \eqref{eq:results} are reliable, and secondly, that the vertex creation algorithm is decoupled from the other algorithms in the reconstruction chain; however, improving this algorithm results in enhanced performance across the whole reconstruction chain. 

\subsection{Future outlook: improving the vertex creation algorithm}

Given that the vertex reconstruction plays a pivotal role in the event reconstruction chain, as previously demonstrated, and that there is a potential ${\sim}\SI{8}{\percent}$ efficiency gain that can be realised by enhancing this specific reconstruction step, this section endeavours to outline the subsequent steps necessary to achieve this objective. 
This will surely prove core for the $\PGnGm$CC QE Np analysis channel, but also for many future analyses, such as a $\PGne$CC QE analysis, for which preliminary work is now being performed \cite{Triozzi:2025_impactNueReconstruction}. 

To improve the vertex reconstruction, two strategies are possible. One might take the currently existing algorithms, which are described in \autoref{sec:TPC_reco_gen}, and fine-tune their parameters and retrain the vertex selection algorithm: this is a valid solution, but this work highlights that the maximum improvement which can be achieved in this way would result in a ${\sim}\SI{5}{\percent}$ boost in the event reconstruction efficiency. 
Another possibility is represented by newer software algorithms, currently being developed within Pandora software framework and tested in other LArTPC experiments, such as DUNE. Such tools employ the use of newer and improved machine-learning-based technologies, such as Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). 

The idea of the deep learning vertex creation algorithm \cite{DUNE:2025wti} arises from two considerations. The first is that the filtered hits on the readout planes taken as input for Pandora topological event reconstruction can be assimilated to 2D images, where each hit is a point on the image itself. The second is that, upon a visual inspection of the images of the neutrino interaction by a human, the interaction vertex is often, though not always, easily identified. Such a premise suggests that it is reasonable to assume that a machine-learning-based algorithm, employing deep convolutional neural networks, often used in the context of medical imaging \cite{ronneberger2015unetconvolutionalnetworksbiomedical,10.1007/978-3-319-24574-4_28}, would be well suited to serve this inherently visual task. 

The concept for this network design adopted in this context is to relate each hit to the distance from the interaction vertex: each hit contributes to the identification of the interaction vertex, and the network can learn reciprocal hit spatial correlations, providing context to the vertex identification. If the hit is defined as a pixel point of coordinates $(h_x, h_c)$, with $x$ being the drift coordinate and $c$ being the readout channel in $u$, $v$, and $w$ views, and the same is done for the true interaction vertex, $(v_x, v_c)$, it is possible to define as a distance metric \begin{equation}
    D = \frac{\sqrt{\qty(v_x - h_x)^2 + \qty(v_c - h_c)^2}}{\floor{\sqrt{2\qty(L - 1)}}}.
\end{equation} It should be noted that this definition of distance is scale-invariant, since the denominator accounts for the characteristic dimension of the interaction L. This is core to prevent the network from learning any scale-related feature (i.e., not being correlated with the interaction characteristic dimension). The computed distance is then allocated into one of 19 classes: in this context, a ``class'' is a label that is added to each hit in the interaction; the network target is to learn this semantic label and, during inference, to assign the correct class (for further details, see \autoref{fig:DLVertexAlgorithm}\ref{sub@fig:DLVertexAlgorithmClasses}). The network then tries to infer the class for each hit. Once the classes are inferred for each hit in the interaction, the class number provides each hit the information of its relative distance to the interaction vertex. This information is used to project rings of appropriate inner and outer radii, centred on each hit. The width of the rings depends on the distance of the hit from the reconstructed vertex and is useful to provide a weight for each hit: the furthest hits are supposedly less spatially correlated with the interaction features closer to the vertex; therefore, the accuracy in their distance to the vertex is supposed to be less. In \autoref{fig:DLVertexAlgorithm}\ref{sub@fig:DLVertexAlgorithmHeatmap} the projected rings for three hits in the interaction are shown as an example. The reconstructed interaction vertex lies in the common region where the rings cross. 

\begin{figure}
    \centering
    \subfloat[]{\includegraphics[width=0.5\linewidth]{pandora/vertex_DL/vtx_classes.png}\label{fig:DLVertexAlgorithmClasses}}
    \subfloat[]{\includegraphics[width=0.5\linewidth]{pandora/vertex_DL/vtx_infer.png}\label{fig:DLVertexAlgorithmHeatmap}}
    \caption[Working principle of the DL vertex algorithm]{\ref{sub@fig:DLVertexAlgorithmClasses} shows the input hits and assignment of the first seven of the nineteen true distance classes for those hits. \ref{sub@fig:DLVertexAlgorithmHeatmap} shows a schematic of the heat map produced by three arbitrary hits during inference, for one view ($w$) of an event. }
    \label{fig:DLVertexAlgorithm}
\end{figure}

Further details on the implementation of the deep learning (DL) vertex algorithm are presented in Ref. \cite{DUNE:2025wti}. 

Preliminary tests and applications in DUNE show remarkable improvements in the vertex reconstruction; however, implementing it properly for the case of ICARUS requires caution. Firstly, it is important to remark that it was originally developed for use within DUNE, where the rate of cosmic-ray interaction is significantly lower. This means that a lower rate of cosmic contamination in neutrino interaction is present, helping the performance of the algorithm. This in turn implies that in order to both train the algorithm and then use it in inference with real data, some upgrades are required to the reconstruction chain to ensure that slices are created correctly and no particle is split into multiple reconstructed slices, i.e., into different reconstructed interactions.

The analysis conducted in this work, as detailed in \autoref{sec:methods}, yielded the results presented in Eq. \eqref{eq:results}. These findings indicated that enhancing the vertex reconstruction process and addressing the approximately \SI{8}{\percent} inefficiencies caused by its misidentification are essential. In this section, we outlined the subsequent steps involved, employing a novel approach for vertex reconstruction and outlining the other necessary steps to implement this revised algorithm.

However, the vertex reconstruction is not the only step that, given the results in Eq. \eqref{eq:results}, has a significant impact on the reconstruction performance. The cluster creation is also strongly influencing the performance of the event reconstruction. This is a more intricate step, which is addressed not only by the dedicated tools performing the cluster creation, but also by the tools involved in the three-dimensional reconstruction that perform the refinements of the clusters, as highlighted in the dedicated subsection in \autoref{sec:TPC_reco_gen}. Therefore, plans to also perform improvements toward a more efficient cluster creation are foreseen, but given the complexity of the task, they require a more detailed study. 

% \input{3_mainmatter/4.2_sliceIssue}

